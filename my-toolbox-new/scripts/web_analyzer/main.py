import argparse
import os
from pathlib import Path
try:
    import requests
    from bs4 import BeautifulSoup
    from snownlp import SnowNLP
    from wordcloud import WordCloud
    import jieba
except ImportError as e:
    print(f'é”™è¯¯ï¼šç¼ºå°‘å¿…è¦çš„ä¾èµ–åŒ… -> {e.name}')
    print('è¯·åœ¨å·¥å…·ç®±ä¸­ä¸ºæ­¤è„šæœ¬é…ç½®è™šæ‹Ÿç¯å¢ƒï¼Œå¹¶å®‰è£…æ‰€æœ‰å£°æ˜çš„ä¾èµ–ã€‚')
    exit(1)

def get_metadata():
    """è¿”å›è„šæœ¬çš„å…ƒæ•°æ®ï¼Œä¾›å·¥å…·ç®±è‡ªåŠ¨ç”ŸæˆUIç•Œé¢ã€‚"""
    return {'name': 'web_analyzer', 'description': 'è¾“å…¥ä¸€ä¸ªç½‘å€ï¼Œæå–æ­£æ–‡è¿›è¡Œæƒ…æ„Ÿåˆ†æï¼Œå¹¶ç”Ÿæˆä¸­æ–‡è¯äº‘å›¾ã€‚', 'dependencies': ['requests', 'beautifulsoup4', 'snownlp', 'wordcloud', 'jieba'], 'parameters': [{'name': 'url', 'label': 'ç›®æ ‡ç½‘é¡µURL', 'type': 'text', 'required': True, 'placeholder': 'ä¾‹å¦‚ï¼šhttps://www.bbc.com/zhongwen/simp'}, {'name': 'output_path', 'label': 'è¯äº‘å›¾ä¿å­˜ä½ç½®', 'type': 'folder', 'required': True, 'placeholder': 'é€‰æ‹©ä¸€ä¸ªæ–‡ä»¶å¤¹ç”¨äºä¿å­˜ç”Ÿæˆçš„å›¾ç‰‡'}, {'name': 'filename', 'label': 'è¾“å‡ºæ–‡ä»¶å', 'type': 'text', 'defaultValue': 'wordcloud.png', 'placeholder': 'ä¾‹å¦‚ï¼šmy_word_cloud.png'}, {'name': 'font_path', 'label': 'ä¸­æ–‡å­—ä½“æ–‡ä»¶è·¯å¾„ (å¯é€‰)', 'type': 'file', 'required': False, 'placeholder': 'è‹¥ä¸æŒ‡å®šï¼Œç¨‹åºä¼šå°è¯•ä½¿ç”¨é»˜è®¤å­—ä½“'}], 'category': 'æµ‹è¯•', 'icon': 'C:\\Users\\Night Shang\\Desktop\\gjx\\my-toolbox-new\\assets\\icons\\icon-mo.ico'}

def main():
    """ä¸»æ‰§è¡Œå‡½æ•°"""
    parser = argparse.ArgumentParser(description='åˆ†æç½‘é¡µå†…å®¹å¹¶ç”Ÿæˆè¯äº‘å›¾ã€‚')
    parser.add_argument('--url', type=str, required=True, help='ç›®æ ‡ç½‘é¡µURL')
    parser.add_argument('--output_path', type=str, required=True, help='è¯äº‘å›¾ç‰‡ä¿å­˜æ–‡ä»¶å¤¹')
    parser.add_argument('--filename', type=str, default='wordcloud.png', help='è¾“å‡ºå›¾ç‰‡çš„æ–‡ä»¶å')
    parser.add_argument('--font_path', type=str, default=None, help='ä¸­æ–‡å­—ä½“æ–‡ä»¶è·¯å¾„')
    args = parser.parse_args()
    print(f'ğŸš€ å¼€å§‹åˆ†æç½‘é¡µ: {args.url}')
    try:
        print('ç¬¬ä¸€æ­¥ï¼šæ­£åœ¨è·å–ç½‘é¡µå†…å®¹...')
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
        response = requests.get(args.url, headers=headers, timeout=15)
        response.raise_for_status()
        response.encoding = response.apparent_encoding
        print('âœ… å†…å®¹è·å–æˆåŠŸï¼')
        print('\nç¬¬äºŒæ­¥ï¼šæ­£åœ¨ä½¿ç”¨ BeautifulSoup æå–æ­£æ–‡...')
        soup = BeautifulSoup(response.text, 'html.parser')
        paragraphs = soup.find_all('p')
        text = '\n'.join([p.get_text() for p in paragraphs])
        if not text:
            print('âš ï¸ è­¦å‘Šï¼šæœªèƒ½ä» <p> æ ‡ç­¾ä¸­æå–åˆ°æœ‰æ•ˆæ–‡æœ¬ï¼Œå°è¯•æå–æ•´ä¸ªbodyã€‚')
            text = soup.body.get_text()
        if not text.strip():
            print('âŒ é”™è¯¯ï¼šæ— æ³•ä»ç½‘é¡µä¸­æå–ä»»ä½•æœ‰æ•ˆæ–‡æœ¬ã€‚\n')
            return
        print('âœ… æ–‡æœ¬æå–æˆåŠŸï¼')
        print('\nç¬¬ä¸‰æ­¥ï¼šæ­£åœ¨ä½¿ç”¨ SnowNLP è¿›è¡Œæƒ…æ„Ÿåˆ†æ...')
        s = SnowNLP(text)
        sentiment_score = s.sentiments
        sentiment_text = 'æ­£é¢' if sentiment_score > 0.6 else 'è´Ÿé¢' if sentiment_score < 0.4 else 'ä¸­æ€§'
        print(f'âœ… æƒ…æ„Ÿåˆ†æå®Œæˆï¼å€¾å‘å¾—åˆ†: {sentiment_score:.4f} ({sentiment_text})')
        print('\nç¬¬å››æ­¥ï¼šæ­£åœ¨ä½¿ç”¨ jieba å’Œ wordcloud ç”Ÿæˆè¯äº‘å›¾...')
        word_list = jieba.cut(text)
        segmented_text = ' '.join(word_list)
        font_path = args.font_path
        if font_path and (not os.path.exists(font_path)):
            print(f'âš ï¸ è­¦å‘Šï¼šæä¾›çš„å­—ä½“è·¯å¾„ä¸å­˜åœ¨: {font_path}ã€‚å°†å°è¯•ä½¿ç”¨é»˜è®¤å­—ä½“ã€‚\n')
            font_path = None
        if not font_path:
            print('â„¹ï¸ æç¤ºï¼šæœªæä¾›æœ‰æ•ˆçš„ä¸­æ–‡å­—ä½“è·¯å¾„ã€‚\n')
            print('å¦‚æœç”Ÿæˆçš„è¯äº‘å›¾ä¸­ä¸­æ–‡æ˜¾ç¤ºä¸ºæ–¹æ¡†ï¼Œè¯·åœ¨å‚æ•°ä¸­æŒ‡å®šä¸€ä¸ªæœ‰æ•ˆçš„ä¸­æ–‡å­—ä½“æ–‡ä»¶ï¼ˆ.ttf æˆ– .otfï¼‰ã€‚\n')
            print('ä¾‹å¦‚ï¼šC:\\Windows\\Fonts\\msyh.ttc (å¾®è½¯é›…é»‘)\n')
        try:
            wc = WordCloud(width=800, height=600, background_color='white', font_path=font_path, scale=1.5).generate(segmented_text)
            output_file = Path(args.output_path) / args.filename
            print(f'\nç¬¬äº”æ­¥ï¼šæ­£åœ¨ä¿å­˜å›¾ç‰‡åˆ°: {output_file}\n')
            wc.to_file(output_file)
            print(f'\nğŸ‰ ä»»åŠ¡å®Œæˆï¼è¯äº‘å›¾å·²æˆåŠŸä¿å­˜ã€‚\n')
        except Exception as e:
            print(f'âŒ ç”Ÿæˆè¯äº‘å›¾æ—¶å‡ºé”™: {e}\n')
            print('âŒ è¿™é€šå¸¸æ˜¯å› ä¸ºæ²¡æœ‰æ‰¾åˆ°åˆé€‚çš„ä¸­æ–‡å­—ä½“ã€‚è¯·å°è¯•åœ¨å‚æ•°ä¸­æ˜ç¡®æŒ‡å®šä¸€ä¸ªå­—ä½“æ–‡ä»¶è·¯å¾„ã€‚\n')
    except requests.exceptions.RequestException as e:
        print(f'âŒ ç½‘ç»œè¯·æ±‚å¤±è´¥: {e}\n')
    except Exception as e:
        print(f'âŒ å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}\n')
if __name__ == '__main__':
    main()